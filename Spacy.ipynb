{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: spacy in c:\\users\\valen\\anaconda3\\lib\\site-packages (3.0.5)\n",
      "Requirement already satisfied, skipping upgrade: typer<0.4.0,>=0.3.0 in c:\\users\\valen\\anaconda3\\lib\\site-packages (from spacy) (0.3.2)\n",
      "Requirement already satisfied, skipping upgrade: wasabi<1.1.0,>=0.8.1 in c:\\users\\valen\\anaconda3\\lib\\site-packages (from spacy) (0.8.2)\n",
      "Requirement already satisfied, skipping upgrade: spacy-legacy<3.1.0,>=3.0.0 in c:\\users\\valen\\anaconda3\\lib\\site-packages (from spacy) (3.0.2)\n",
      "Requirement already satisfied, skipping upgrade: murmurhash<1.1.0,>=0.28.0 in c:\\users\\valen\\anaconda3\\lib\\site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied, skipping upgrade: preshed<3.1.0,>=3.0.2 in c:\\users\\valen\\anaconda3\\lib\\site-packages (from spacy) (3.0.5)\n",
      "Requirement already satisfied, skipping upgrade: srsly<3.0.0,>=2.4.0 in c:\\users\\valen\\anaconda3\\lib\\site-packages (from spacy) (2.4.1)\n",
      "Requirement already satisfied, skipping upgrade: tqdm<5.0.0,>=4.38.0 in c:\\users\\valen\\anaconda3\\lib\\site-packages (from spacy) (4.50.2)\n",
      "Requirement already satisfied, skipping upgrade: requests<3.0.0,>=2.13.0 in c:\\users\\valen\\anaconda3\\lib\\site-packages (from spacy) (2.24.0)\n",
      "Requirement already satisfied, skipping upgrade: pydantic<1.8.0,>=1.7.1 in c:\\users\\valen\\anaconda3\\lib\\site-packages (from spacy) (1.7.3)\n",
      "Requirement already satisfied, skipping upgrade: jinja2 in c:\\users\\valen\\anaconda3\\lib\\site-packages (from spacy) (2.11.2)\n",
      "Requirement already satisfied, skipping upgrade: blis<0.8.0,>=0.4.0 in c:\\users\\valen\\anaconda3\\lib\\site-packages (from spacy) (0.7.4)\n",
      "Requirement already satisfied, skipping upgrade: thinc<8.1.0,>=8.0.2 in c:\\users\\valen\\anaconda3\\lib\\site-packages (from spacy) (8.0.2)\n",
      "Requirement already satisfied, skipping upgrade: packaging>=20.0 in c:\\users\\valen\\anaconda3\\lib\\site-packages (from spacy) (20.4)\n",
      "Requirement already satisfied, skipping upgrade: cymem<2.1.0,>=2.0.2 in c:\\users\\valen\\anaconda3\\lib\\site-packages (from spacy) (2.0.5)\n",
      "Requirement already satisfied, skipping upgrade: setuptools in c:\\users\\valen\\anaconda3\\lib\\site-packages (from spacy) (50.3.1.post20201107)\n",
      "Requirement already satisfied, skipping upgrade: pathy>=0.3.5 in c:\\users\\valen\\anaconda3\\lib\\site-packages (from spacy) (0.4.0)\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.15.0 in c:\\users\\valen\\anaconda3\\lib\\site-packages (from spacy) (1.19.2)\n",
      "Requirement already satisfied, skipping upgrade: catalogue<2.1.0,>=2.0.1 in c:\\users\\valen\\anaconda3\\lib\\site-packages (from spacy) (2.0.1)\n",
      "Requirement already satisfied, skipping upgrade: click<7.2.0,>=7.1.1 in c:\\users\\valen\\anaconda3\\lib\\site-packages (from typer<0.4.0,>=0.3.0->spacy) (7.1.2)\n",
      "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in c:\\users\\valen\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2020.6.20)\n",
      "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in c:\\users\\valen\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
      "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\valen\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.25.11)\n",
      "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in c:\\users\\valen\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
      "Requirement already satisfied, skipping upgrade: MarkupSafe>=0.23 in c:\\users\\valen\\anaconda3\\lib\\site-packages (from jinja2->spacy) (1.1.1)\n",
      "Requirement already satisfied, skipping upgrade: pyparsing>=2.0.2 in c:\\users\\valen\\anaconda3\\lib\\site-packages (from packaging>=20.0->spacy) (2.4.7)\n",
      "Requirement already satisfied, skipping upgrade: six in c:\\users\\valen\\anaconda3\\lib\\site-packages (from packaging>=20.0->spacy) (1.15.0)\n",
      "Requirement already satisfied, skipping upgrade: smart-open<4.0.0,>=2.2.0 in c:\\users\\valen\\anaconda3\\lib\\site-packages (from pathy>=0.3.5->spacy) (3.0.0)\n",
      "Requirement already up-to-date: spacy-lookups-data in c:\\users\\valen\\anaconda3\\lib\\site-packages (1.0.0)\n",
      "Requirement already satisfied, skipping upgrade: setuptools in c:\\users\\valen\\anaconda3\\lib\\site-packages (from spacy-lookups-data) (50.3.1.post20201107)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Python \n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "[E050] Can't find model 'en_core_web_sm'. It doesn't seem to be a Python package or a valid path to a data directory.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-3946f0887fa2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'python3 -m spacy download en_core_web_s'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mspacy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mnlp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mspacy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"en_core_web_sm\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\spacy\\__init__.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(name, disable, exclude, config)\u001b[0m\n\u001b[0;32m     45\u001b[0m     \u001b[0mRETURNS\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mLanguage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mThe\u001b[0m \u001b[0mloaded\u001b[0m \u001b[0mnlp\u001b[0m \u001b[0mobject\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m     \"\"\"\n\u001b[1;32m---> 47\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mutil\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdisable\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdisable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexclude\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mexclude\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     48\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\spacy\\util.py\u001b[0m in \u001b[0;36mload_model\u001b[1;34m(name, vocab, disable, exclude, config)\u001b[0m\n\u001b[0;32m    327\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mOLD_MODEL_SHORTCUTS\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    328\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mE941\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfull\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mOLD_MODEL_SHORTCUTS\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 329\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mE050\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    330\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    331\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mOSError\u001b[0m: [E050] Can't find model 'en_core_web_sm'. It doesn't seem to be a Python package or a valid path to a data directory."
     ]
    }
   ],
   "source": [
    "!pip3 install -U spacy\n",
    "!pip3 install -U spacy-lookups-data\n",
    "!python3 -m spacy download en_core_web_s\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"While Samsung has expanded overseas, South Korea is still host to most of its factories and research engineers.\")\n",
    "print(\"(текст токена, начальная форма, часть речи, стоп-слово?)\")\n",
    "for token in doc:\n",
    "    print(token.text, token.lemma_, token.pos_, token.is_stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# дерево зависимостей\n",
    "for token in doc:\n",
    "    print(token.text, token.dep_, token.head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# визуализация дерева зависимости\n",
    "from spacy import displacy\n",
    "displacy.render(doc[:11], style='dep', jupyter=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# именнованные сущности\n",
    "doc2 = nlp(\"Nasa administrator Jim Bridenstine says at the moment of launch, he was praying.\")\n",
    "for ent in doc2.ents:\n",
    "    print(ent.text, ent.label_)\n",
    "displacy.render(doc2, style='ent', jupyter=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-3-c3801ab0847d>, line 58)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-3-c3801ab0847d>\"\u001b[1;36m, line \u001b[1;32m58\u001b[0m\n\u001b[1;33m    print \"models/ntype_prediction\" + self.bd +\"ts\"+str(self.trainset_p)+self.param+str(val)+\"k\"+str(k)+\"Promedio\"+str(self.iteraciones)+\".p\"\u001b[0m\n\u001b[1;37m          ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import pydot \n",
    "from node2vec import *\n",
    "from sklearn import neighbors,metrics\n",
    "from credentials import *\n",
    "import sys    # sys.setdefaultencoding is cancelled by site.py\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.cross_validation import StratifiedKFold\n",
    "reload(sys)    # to re-enable sys.setdefaultencoding()\n",
    "sys.setdefaultencoding('utf-8')\n",
    "import multiprocessing\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.neighbors.kde import KernelDensity\n",
    "from sklearn.tree import DecisionTreeClassifier, export_graphviz\n",
    "\n",
    "#el valor de trainset_p sera usado como probabilidad de que un elemento sea evaluado con knn (valor entre 0 y 1 ) o como valor de la cantidad de folds en el cross valdiation para cualquier de los otros metodos.\n",
    "class experiment:\n",
    "    def __init__(self,bd,port,user,pss,label,mode,param,trainset_p,iteraciones):\n",
    "        self.bd = bd \n",
    "        self.mode = mode \n",
    "        self.port = port\n",
    "        self.user = user\n",
    "        self.pss = pss\n",
    "        self.label = label\n",
    "        self.trainset_p = trainset_p\n",
    "        self.param = param\n",
    "        self.p = figure(plot_width=600, plot_height=400)    \n",
    "        self.ratiosf = {}\n",
    "        self.r_desv = {}\n",
    "        self.n_desv = {}\n",
    "        self.iteraciones = iteraciones\n",
    "\n",
    "    def ntype_prediction(self,a,b,jump):\n",
    "        pal = pallete(\"db\")\n",
    "        # Valores para la grafica de precision en la prediccion\n",
    "        X = []\n",
    "        Y = []\n",
    "        # Valores para la grafica de desviacion en la prediccion\n",
    "        Xd = []\n",
    "        Yd = []\n",
    "        i = 1\n",
    "        for i in range(a,b+1):\n",
    "            val = i * jump  \n",
    "            if self.param == \"k\":\n",
    "                val = val - 1\n",
    "            if self.param == \"ns\":\n",
    "                k = 3\n",
    "            if self.param == \"l\":\n",
    "                k = 3\n",
    "            if self.param == \"ndim\":\n",
    "                k = 3\n",
    "            if not (self.param == \"ns\" or self.param == \"ndim\" or self.param == \"l\"):\n",
    "                k = val\n",
    "            resultados = []     \n",
    "            print \"models/ntype_prediction\" + self.bd +\"ts\"+str(self.trainset_p)+self.param+str(val)+\"k\"+str(k)+\"Promedio\"+str(self.iteraciones)+\".p\"\n",
    "            print os.path.exists(\"models/ntype_prediction\" + self.bd +\"ts\"+str(self.trainset_p)+self.param+str(val)+\"k\"+str(k)+\"Promedio\"+str(self.iteraciones)+\".p\")   \n",
    "            if not os.path.exists(\"models/ntype_prediction\" + self.bd +\"ts\"+str(self.trainset_p)+self.param+str(val)+\"k\"+str(k)+\"Promedio\"+str(self.iteraciones)+\".p\") or not os.path.exists(\"models/ntype_prediction\" + self.bd +\"ts\"+str(self.trainset_p)+self.param+str(val)+\"k\"+str(k)+\"Resultados\"+str(self.iteraciones)+\".p\") or not os.path.exists(\"models/ntype_prediction\" + self.bd +\"ts\"+str(self.trainset_p)+self.param+str(val)+\"k\"+str(k)+\"MeanDev\"+str(self.iteraciones)+\".p\"):\n",
    "                t = 0\n",
    "                for it in range(self.iteraciones):\n",
    "                    if self.param == \"ns\":\n",
    "                        n2v = node2vec(self.bd,self.port,self.user,self.pss,self.label,val,200,6,self.mode,[],self.iteraciones)\n",
    "                        k = 3\n",
    "                    if self.param == \"l\":\n",
    "                        n2v = node2vec(self.bd,self.port,self.user,self.pss,self.label,400000,200,val,self.mode,[],self.iteraciones)\n",
    "                        k = 3\n",
    "                    if self.param == \"ndim\":\n",
    "                        n2v = node2vec(self.bd,self.port,self.user,self.pss,self.label,400000,val,6,self.mode,[],self.iteraciones)\n",
    "                        k = 3\n",
    "                    #si lo que vamos a estudiar no son los parametros libres de la inmersion, fijamos dichos parametros a sus valores optimos segun BD\n",
    "                    if not (self.param == \"ns\" or self.param == \"ndim\" or self.param == \"l\"):\n",
    "                        n2v = node2vec(self.bd,self.port,self.user,self.pss,self.label,optimos[self.bd][0],optimos[self.bd][1],optimos[self.bd][2],self.mode,[],self.iteraciones)\n",
    "                    n2v.learn(self.mode,self.trainset_p,False,it)\n",
    "                    if self.param == \"ns\" or self.param == \"ndim\" or  self.param == \"l\":\n",
    "                        result = predict(\"k\",n2v.nodes_pos,n2v.nodes_type,val,self.trainset_p)\n",
    "                    else:\n",
    "                        result = predict(self.param,n2v.nodes_pos,n2v.nodes_type,val,self.trainset_p)\n",
    "                    t += result\n",
    "                    resultados.append(result)\n",
    "                    print result\n",
    "                result = t / self.iteraciones\n",
    "                mean_dev = 0\n",
    "                for r in resultados:\n",
    "                    mean_dev += (r - result) * (r - result)\n",
    "                mean_dev = math.sqrt(mean_dev)\n",
    "                f1 = open( \"models/ntype_prediction\" + self.bd +\"ts\"+str(self.trainset_p)+self.param+str(val)+\"k\"+str(k)+\"MeanDev\"+str(self.iteraciones)+\".p\", \"w\" )\n",
    "                pickle.dump(mean_dev,f1)\n",
    "                f2 = open( \"models/ntype_prediction\" + self.bd +\"ts\"+str(self.trainset_p)+self.param+str(val)+\"k\"+str(k)+\"Resultados\"+str(self.iteraciones)+\".p\", \"w\" )\n",
    "                pickle.dump(resultados,f2)\n",
    "                f3 = open( \"models/ntype_prediction\" + self.bd +\"ts\"+str(self.trainset_p)+self.param+str(val)+\"k\"+str(k)+\"Promedio\"+str(self.iteraciones)+\".p\", \"w\" )\n",
    "                pickle.dump(result,f3)\n",
    "            else:\n",
    "                f1 = open( \"models/ntype_prediction\" + self.bd +\"ts\"+str(self.trainset_p)+self.param+str(val)+\"k\"+str(k)+\"MeanDev\"+str(self.iteraciones)+\".p\", \"r\" )\n",
    "                mean_dev = pickle.load(f1)\n",
    "                f2 = open( \"models/ntype_prediction\" + self.bd +\"ts\"+str(self.trainset_p)+self.param+str(val)+\"k\"+str(k)+\"Resultados\"+str(self.iteraciones)+\".p\", \"r\" )\n",
    "                resultados = pickle.load(f2)\n",
    "                f3 = open( \"models/ntype_prediction\" + self.bd +\"ts\"+str(self.trainset_p)+self.param+str(val)+\"k\"+str(k)+\"Promedio\"+str(self.iteraciones)+\".p\", \"r\" )\n",
    "                result = pickle.load(f3)\n",
    "            #print \"RESULT\"\n",
    "            #print result\n",
    "            #print \"RESULTADOS\"\n",
    "            #print resultados\n",
    "            #print \"MEAN DEV\"\n",
    "            #print mean_dev\n",
    "            X.append(val)\n",
    "            Y.append(result*100)\n",
    "            Xd.append(val)\n",
    "            Yd.append(mean_dev)\n",
    "        print self.bd\n",
    "        print \"max accuracy: \" + str(max(Y))\n",
    "        print \"max dev: \" + str(max(Yd))\n",
    "        self.p.line(X, Y, color=pal[1],legend=self.bd,line_width=1.5)\n",
    "        #self.p.line(Xd, Yd, color=pal[1],legend=self.bd + \" dev\",line_width=1.5,line_dash='dotted')\n",
    "        self.p.legend.background_fill_alpha = 0.5\n",
    "        return X,Y,Xd,Yd\n",
    "    \n",
    "    def ntype_conf_matrix(self):\n",
    "        k = 3\n",
    "        print \"models/ntype_conf_matrix\" + self.bd +\"ts\"+str(self.trainset_p)+\"k\"+str(k)+\"Promedio\"+str(self.iteraciones)+\".p\"\n",
    "        if not os.path.exists(\"models/ntype_conf_matrix\" + self.bd +\"ts\"+str(self.trainset_p)+\"k\"+str(k)+\"Promedio\"+str(self.iteraciones)+\".p\"):\n",
    "            matrices = [None] * self.iteraciones\n",
    "            #repetimos para self.iteraciones experimentos\n",
    "            for it in range(self.iteraciones):\n",
    "                n2v = node2vec(self.bd,self.port,self.user,self.pss,self.label,optimos[self.bd][0],optimos[self.bd][1],optimos[self.bd][2],self.mode,[],self.iteraciones)\n",
    "                n2v.learn(self.mode,self.trainset_p,False,it)\n",
    "                #generamos un diccionario para saber las posiciones de cada tipo de nodo en la matriz\n",
    "                dic = dict()\n",
    "                for idx,t in enumerate(n2v.n_types):\n",
    "                    dic[t] = idx\n",
    "                #generamos la matriz para cada experimento\n",
    "                matriz = [0] * (len(n2v.n_types)+1)\n",
    "                for i in range(0,len(n2v.n_types)+1):\n",
    "                    matriz[i] = [0] * (len(n2v.n_types)+1)\n",
    "                    for idx,t in enumerate(n2v.n_types):\n",
    "                        if i == 0:\n",
    "                            matriz[i][idx+1] = t\n",
    "                        else:\n",
    "                            matriz[i][idx] = 0    \n",
    "                for idx,t in enumerate(n2v.n_types):\n",
    "                    matriz[idx+1][0] = t\n",
    "                #k-neighbors for each node\n",
    "                pos = []\n",
    "                types = []\n",
    "                for idx,i in enumerate(n2v.nodes_pos):\n",
    "                    if random.random() < self.trainset_p:\n",
    "                        pos.append(i)\n",
    "                        types.append(n2v.nodes_type[idx])\n",
    "                if len(pos) - 1 < k:\n",
    "                    k1 = len(pos) - 1\n",
    "                else:\n",
    "                    k1 = k\n",
    "                clf = neighbors.KNeighborsClassifier(k1+1, \"uniform\",n_jobs=multiprocessing.cpu_count())\n",
    "                clf.fit(n2v.nodes_pos, n2v.nodes_type)\n",
    "                neigh = clf.kneighbors(pos,return_distance = False)\n",
    "                for idx,n in enumerate(neigh):\n",
    "                    votes = []                    \n",
    "                    for idx1,s in enumerate(neigh[idx][1:]):\n",
    "                        votes.append(n2v.nodes_type[s])\n",
    "                    matriz[dic[types[idx]]+1][dic[max(set(votes), key=votes.count)]+1] +=1\n",
    "                print matriz\n",
    "                matrices[it] = matriz\n",
    "            f = open( \"models/ntype_conf_matrix\" + self.bd +\"ts\"+str(self.trainset_p)+\"k\"+str(k)+\"Promedio\"+str(self.iteraciones)+\".p\", \"w\" )\n",
    "            pickle.dump(matrices,f)\n",
    "        else:\n",
    "            f = open( \"models/ntype_conf_matrix\" + self.bd +\"ts\"+str(self.trainset_p)+\"k\"+str(k)+\"Promedio\"+str(self.iteraciones)+\".p\", \"r\" )\n",
    "            matrices = pickle.load(f)\n",
    "        #calculando la matriz de confusion promedios de n experimentos\n",
    "        matriz_promedio = [None] * (len(matrices[0]))\n",
    "        for i in range(0,len(matrices[0])):\n",
    "            matriz_promedio[i] = [0] * (len(matrices[0]))\n",
    "        for idx,t in enumerate(matrices[0]):\n",
    "            matriz_promedio[0][idx] = t[0]\n",
    "        for idx,t in enumerate(matrices[0]):\n",
    "            matriz_promedio[idx][0] = t[0]\n",
    "        for i in range(1,len(matrices[0])):\n",
    "            for j in range(1,len(matrices[0])):\n",
    "                suma = 0\n",
    "                for m in range(self.iteraciones):\n",
    "                    suma += matrices[m][i][j]\n",
    "                matriz_promedio[i][j] = float(suma)/float(self.iteraciones)\n",
    "        #calculando porcentajes a partir del promedio de frecuencias\n",
    "        for i in range(1,len(matriz_promedio)):\n",
    "            suma = 0\n",
    "            for j in range(1,len(matriz_promedio)):                \n",
    "                suma += matriz_promedio[i][j]\n",
    "            for j in range(1,len(matriz_promedio)):                \n",
    "                matriz_promedio[i][j] = round(float(matriz_promedio[i][j] * 100) / float(suma),2)\n",
    "        return matriz_promedio\n",
    "\n",
    "\n",
    "    #Por ahora esta preparado para recibir solo dos tipos que se solapan!\n",
    "    def nmultitype_conf_matrix(self,tipos,nfolds):\n",
    "        cadena = \"\"\n",
    "        for t in tipos:\n",
    "            cadena += t\n",
    "        if not os.path.exists(\"models/nmultitype_conf_matrix\" + self.bd +\"ts\"+cadena+\"Promedio\"+str(nfolds)+\".p\") or True:\n",
    "            #Creamos la matriz de matrices donde guardaremos los resultados parciales\n",
    "            matrices = [None] * nfolds * nfolds\n",
    "            #Creamos/Recuperamos el modelo Node2Vec\n",
    "            n2v = node2vec(self.bd,self.port,self.user,self.pss,self.label,1000,20,6,self.mode,[],1)\n",
    "            n2v.learn(\"normal\",0,False,0)\n",
    "            #Creamos los arrays X e Y, anadiendo\n",
    "            X = []\n",
    "            Y = []\n",
    "            #Creamos un array de comunes que son los nodos que son a la vez de ambos tipos\n",
    "            comunes = list()\n",
    "            for tipo in tipos:\n",
    "                for n in n2v.n_types[tipo]:\n",
    "                    if n in n2v.w2v:\n",
    "                        X.append(n2v.w2v[n])\n",
    "                        if n in n2v.n_types[tipos[0]] and  n in n2v.n_types[tipos[1]]:\n",
    "                            comunes.append(n2v.w2v[n])\n",
    "                        Y.append(tipo)\n",
    "            #Creamos los k folds estratificados    \n",
    "            X = np.array(X)\n",
    "            Y = np.array(Y)\n",
    "            skf = StratifiedKFold(Y, n_folds=nfolds)\n",
    "            it = 0\n",
    "            kdes = []\n",
    "            for train_index, test_index in skf:\n",
    "                print \"k-fold para kde\"\n",
    "                X_train, X_test = X[train_index], X[test_index]\n",
    "                Y_train, Y_test = Y[train_index], Y[test_index]\n",
    "                Y_test = Y_test.astype('|S64')\n",
    "                #Creamos la funcion de densidad de probabilidad de cada tipo\n",
    "                for t in tipos:\n",
    "                    print \"Creando KDE para el tipo \"+t\n",
    "                    tempX = []\n",
    "                    for idx,n in enumerate(Y_train):\n",
    "                        if n == t:\n",
    "                            tempX.append(X_train[idx])\n",
    "                    #Calculating KDE with the train set\n",
    "                    #use grid search cross-validation to optimize the bandwidth\n",
    "                    #params = {'bandwidth': np.logspace(-1, 1, 10)}\n",
    "                    #grid = GridSearchCV(neighbors.KernelDensity(), params)\n",
    "                    #grid.fit(tempX)\n",
    "                    #print(\"best bandwidth: {0}\".format(grid.best_estimator_.bandwidth))\n",
    "                    # use the best estimator to compute the kernel density estimate\n",
    "                    #kde = grid.best_estimator_\n",
    "                    kde = KernelDensity(kernel='gaussian', bandwidth=0.1)\n",
    "                    kde.fit(tempX)\n",
    "                    kdes.append(kde)\n",
    "                    print \"Terminado KDE para el tipo \"+t\n",
    "                #Dividimos el conjunto de test en tipo1, tipo2 y tipo1+2\n",
    "                cont = 0\n",
    "                for idx,x in enumerate(X_test):\n",
    "                    total = 0\n",
    "                    x = np.array(x)\n",
    "                    if any((x == a).all() for a in comunes):\n",
    "                        Y_test[idx] = str(tipos[0]+\"+\"+tipos[1])\n",
    "                        cont += 1\n",
    "                print \"Numero de elementos con doble tipo:\"+str(cont)\n",
    "                #Creamos k-folds estratificados para el arbol de decision\n",
    "                skf = StratifiedKFold(Y_test, n_folds=nfolds)\n",
    "                for train_index, test_index in skf:\n",
    "                    print \"k-fold para decission tree\"\n",
    "                    X_train1, X_test1 = X_test[train_index], X_test[test_index]\n",
    "                    Y_train1, Y_test1 = Y_test[train_index], Y_test[test_index]\n",
    "                    clf = DecisionTreeClassifier(random_state=0)\n",
    "                    print X_train1[0]\n",
    "                    clf.fit(X_train1,Y_train1)\n",
    "                    export_graphviz(clf);\n",
    "                    Y_pred1 = clf.predict(X_test1)\n",
    "                    matriz = metrics.confusion_matrix(Y_test1, Y_pred1,[tipos[0],tipos[1],tipos[0]+\"+\"+tipos[1]])\n",
    "                    matrices[it] = np.array(matriz)\n",
    "                    print matrices[it]\n",
    "                    it += 1\n",
    "            f = open( \"models/nmultitype_conf_matrix\" + self.bd +\"ts\"+cadena+\"Promedio\"+str(nfolds)+\".p\", \"w\" )\n",
    "            pickle.dump(matrices,f)\n",
    "        else:\n",
    "            f = open( \"models/nmultitype_conf_matrix\" + self.bd +\"ts\"+cadena+\"Promedio\"+str(nfolds)+\".p\", \"r\" )\n",
    "            matrices = pickle.load(f)\n",
    "        total = matrices[0]\n",
    "        for m in matrices[1:]:\n",
    "            total += m\n",
    "        print total\n",
    "        matriz_promedio = total \n",
    "        matriz_promedio = matriz_promedio.astype('float')\n",
    "        #print matrices\n",
    "        #print matriz_promedio\n",
    "        matriz_promedio = matriz_promedio / len(matrices)\n",
    "        #print matriz_promedio\n",
    "        #calculando porcentajes a partir del promedio de frecuencias\n",
    "        for i in range(0,len(matriz_promedio)):\n",
    "            suma = 0\n",
    "            for j in range(0,len(matriz_promedio)): \n",
    "                suma += matriz_promedio[i][j]\n",
    "                matriz_promedio[i][j] = float(matriz_promedio[i][j])\n",
    "            for j in range(0,len(matriz_promedio)):                \n",
    "                if suma > 0:\n",
    "                    matriz_promedio[i][j] = round(float(matriz_promedio[i][j] * 100) / float(suma),2)\n",
    "                else:\n",
    "                    matriz_promedio[i][j] = 0\n",
    "        matriz_promedio = matriz_promedio.astype('string')\n",
    "        for i in range(0,len(matriz_promedio)):\n",
    "            for j in range(0,len(matriz_promedio)):\n",
    "                matriz_promedio[i][j] = str(matriz_promedio[i][j])+\"%\"\n",
    "        return matriz_promedio\n",
    "\n",
    "\n",
    "\n",
    "    def ltype_prediction(self,a,b,jump):\n",
    "        # Valores para la grafica de precision en la prediccion\n",
    "        pal = pallete(\"db\")\n",
    "        X = []\n",
    "        Y = []\n",
    "        # Valores para la grafica de desviacion en la prediccion\n",
    "        Xd = []\n",
    "        Yd = []\n",
    "        i = 1\n",
    "        for i in range(a,b+1):\n",
    "            val = i * jump    \n",
    "            if self.param == \"k\":\n",
    "                val = val - 1\n",
    "            if self.param == \"ns\":\n",
    "                k = 3\n",
    "            if self.param == \"l\":\n",
    "                k = 3\n",
    "            if self.param == \"ndim\":\n",
    "                k = 3\n",
    "            resultados = []                \n",
    "            if not os.path.exists(\"models/ltype_prediction\" + self.bd +\"ts\"+str(self.trainset_p)+self.param+str(val)+\"k\"+str(k)+\"Promedio\"+str(self.iteraciones)+\".p\") or not os.path.exists(\"models/ltype_prediction\" + self.bd +\"ts\"+str(self.trainset_p)+self.param+str(val)+\"k\"+str(k)+\"Resultados\"+str(self.iteraciones)+\".p\") or not os.path.exists(\"models/ltype_prediction\" + self.bd +\"ts\"+str(self.trainset_p)+self.param+str(val)+\"k\"+str(k)+\"MeanDev\"+str(self.iteraciones)+\".p\"):\n",
    "                final = 0\n",
    "                for it in range(self.iteraciones):\n",
    "                    if self.param == \"ns\":\n",
    "                        n2v = node2vec(self.bd,self.port,self.user,self.pss,self.label,val,200,6,self.mode,[],self.iteraciones)\n",
    "                        k = 3\n",
    "                    if self.param == \"l\":\n",
    "                        n2v = node2vec(self.bd,self.port,self.user,self.pss,self.label,400000,200,val,self.mode,[],self.iteraciones)\n",
    "                        k = 3\n",
    "                    if self.param == \"ndim\":\n",
    "                        n2v = node2vec(self.bd,self.port,self.user,self.pss,self.label,400000,val,6,self.mode,[],self.iteraciones)\n",
    "                        k = 3\n",
    "                    #si lo que vamos a estudiar no son los parametros libres de la inmersion, fijamos dichos parametros a sus valores optimos segun BD\n",
    "                    if not (self.param == \"ns\" or self.param == \"ndim\" or self.param == \"l\"):\n",
    "                        n2v = node2vec(self.bd,self.port,self.user,self.pss,self.label,optimos[self.bd][0],optimos[self.bd][1],optimos[self.bd][2],self.mode,[],self.iteraciones)\n",
    "                    n2v.learn(self.mode,self.trainset_p,False,it)\n",
    "                    #k-neighbors for each node\n",
    "                    total = 0\n",
    "                    right = 0\n",
    "                    link_vectors = []\n",
    "                    link_types = []\n",
    "                    for t in n2v.r_types:\n",
    "                        for r in n2v.r_types[t]:\n",
    "                            link_vectors.append(r[\"v\"])\n",
    "                            link_types.append(t)\n",
    "                    if self.param == \"ns\" or self.param == \"ndim\" or  self.param == \"l\":\n",
    "                        result = predict(\"k\",link_vectors,link_types,k,self.trainset_p)\n",
    "                    else:\n",
    "                        result = predict(self.param,link_vectors,link_types,val,self.trainset_p)\n",
    "                    final += result\n",
    "                    resultados.append(result)\n",
    "                result = final / self.iteraciones                \n",
    "                mean_dev = 0\n",
    "                for r in resultados:\n",
    "                    mean_dev += (r - result) * (r - result)\n",
    "                mean_dev = math.sqrt(mean_dev)\n",
    "                f1 = open( \"models/ltype_prediction\" + self.bd +\"ts\"+str(self.trainset_p)+self.param+str(val)+\"k\"+str(k)+\"MeanDev\"+str(self.iteraciones)+\".p\", \"w\" )\n",
    "                pickle.dump(mean_dev,f1)\n",
    "                f2 = open( \"models/ltype_prediction\" + self.bd +\"ts\"+str(self.trainset_p)+self.param+str(val)+\"k\"+str(k)+\"Resultados\"+str(self.iteraciones)+\".p\", \"w\" )\n",
    "                pickle.dump(resultados,f2)\n",
    "                f3 = open( \"models/ltype_prediction\" + self.bd +\"ts\"+str(self.trainset_p)+self.param+str(val)+\"k\"+str(k)+\"Promedio\"+str(self.iteraciones)+\".p\", \"w\" )\n",
    "                pickle.dump(result,f3)\n",
    "            else:\n",
    "                f1 = open( \"models/ltype_prediction\" + self.bd +\"ts\"+str(self.trainset_p)+self.param+str(val)+\"k\"+str(k)+\"MeanDev\"+str(self.iteraciones)+\".p\", \"r\" )\n",
    "                mean_dev = pickle.load(f1)\n",
    "                f2 = open( \"models/ltype_prediction\" + self.bd +\"ts\"+str(self.trainset_p)+self.param+str(val)+\"k\"+str(k)+\"Resultados\"+str(self.iteraciones)+\".p\", \"r\" )\n",
    "                resultados = pickle.load(f2)\n",
    "                f3 = open( \"models/ltype_prediction\" + self.bd +\"ts\"+str(self.trainset_p)+self.param+str(val)+\"k\"+str(k)+\"Promedio\"+str(self.iteraciones)+\".p\", \"r\" )\n",
    "                result = pickle.load(f3)\n",
    "            X.append(val)\n",
    "            Y.append(result*100)\n",
    "            Xd.append(val)\n",
    "            Yd.append(mean_dev*100)\n",
    "        self.p.line(X, Y, color=pal[1],legend=\"ICH\",line_width=1.5)\n",
    "        #self.p.line(Xd, Yd, color=pal[1],legend=\"ICH\",line_width=1.5,line_dash='dotted')\n",
    "        self.p.legend.background_fill_alpha = 0.5\n",
    "        print self.bd\n",
    "        print \"max accuracy: \" + str(max(Y))\n",
    "        print \"max dev: \" + str(max(Yd))\n",
    "        return X,Y,Xd,Yd\n",
    "\n",
    "    def ltype_conf_matrix(self):\n",
    "        k = 3\n",
    "        if not os.path.exists(\"models/ltype_conf_matrix\" + self.bd +\"ts\"+str(self.trainset_p)+\"k\"+str(k)+\"Promedio\"+str(self.iteraciones)+\".p\"):\n",
    "            matrices = [None] * self.iteraciones\n",
    "            #repetimos para self.iteraciones experimentos\n",
    "            for it in range(self.iteraciones):\n",
    "                n2v = node2vec(self.bd,self.port,self.user,self.pss,self.label,optimos[self.bd][0],optimos[self.bd][1],optimos[self.bd][2],self.mode,[],self.iteraciones)\n",
    "                n2v.learn(self.mode,self.trainset_p,False,it)\n",
    "                #generamos un diccionario para saber las posiciones de cada tipo de nodo en la matriz\n",
    "                dic = dict()\n",
    "                for idx,t in enumerate(n2v.r_types):\n",
    "                    dic[t] = idx\n",
    "                #generamos la matriz para cada experimento\n",
    "                matriz = [0] * (len(n2v.r_types)+1)\n",
    "                for i in range(0,len(n2v.r_types)+1):\n",
    "                    matriz[i] = [0] * (len(n2v.r_types)+1)\n",
    "                    for idx,t in enumerate(n2v.r_types):\n",
    "                        if i == 0:\n",
    "                            matriz[i][idx+1] = t\n",
    "                        else:\n",
    "                            matriz[i][idx] = 0    \n",
    "                for idx,t in enumerate(n2v.r_types):\n",
    "                    matriz[idx+1][0] = t\n",
    "                #k-neighbors for each node\n",
    "                link_vectors = []\n",
    "                link_types = []\n",
    "                for t in n2v.r_types:\n",
    "                    for r in n2v.r_types[t]:\n",
    "                        link_vectors.append(r[\"v\"])\n",
    "                        link_types.append(t)\n",
    "                if len(link_vectors) - 1 < k:\n",
    "                    k1 = len(link_vectors) - 1\n",
    "                else:\n",
    "                    k1 = k\n",
    "                clf = neighbors.KNeighborsClassifier(k1+1, \"uniform\",n_jobs=multiprocessing.cpu_count())\n",
    "                clf.fit(link_vectors, link_types)\n",
    "                pos = []\n",
    "                types = []\n",
    "                for idx,i in enumerate(link_vectors):\n",
    "                    if random.random() < self.trainset_p:\n",
    "                        pos.append(i)\n",
    "                        types.append(link_types[idx])\n",
    "                neigh = clf.kneighbors(pos,return_distance = False)\n",
    "                for idx,n in enumerate(neigh):\n",
    "                    votes = []                    \n",
    "                    for idx1,s in enumerate(neigh[idx][1:]):\n",
    "                        votes.append(link_types[s])\n",
    "                    matriz[dic[types[idx]]+1][dic[max(set(votes), key=votes.count)]+1] +=1\n",
    "                print matriz\n",
    "                matrices[it] = matriz\n",
    "            f = open( \"models/ltype_conf_matrix\" + self.bd +\"ts\"+str(self.trainset_p)+\"k\"+str(k)+\"Promedio\"+str(self.iteraciones)+\".p\", \"w\" )\n",
    "            pickle.dump(matrices,f)\n",
    "        else:\n",
    "            f = open( \"models/ltype_conf_matrix\" + self.bd +\"ts\"+str(self.trainset_p)+\"k\"+str(k)+\"Promedio\"+str(self.iteraciones)+\".p\", \"r\" )\n",
    "            matrices = pickle.load(f)\n",
    "        #calculando la matriz de confusion promedios de n experimentos\n",
    "        matriz_promedio = [None] * (len(matrices[0]))\n",
    "        for i in range(0,len(matrices[0])):\n",
    "            matriz_promedio[i] = [0] * (len(matrices[0]))\n",
    "        for idx,t in enumerate(matrices[0]):\n",
    "            matriz_promedio[0][idx] = t[0]\n",
    "        for idx,t in enumerate(matrices[0]):\n",
    "            matriz_promedio[idx][0] = t[0]\n",
    "        for i in range(1,len(matrices[0])):\n",
    "            for j in range(1,len(matrices[0])):\n",
    "                suma = 0\n",
    "                for m in range(self.iteraciones):\n",
    "                    suma += matrices[m][i][j]\n",
    "                matriz_promedio[i][j] = float(suma)/float(self.iteraciones)\n",
    "        #calculando porcentajes a partir del promedio de frecuencias\n",
    "        for i in range(1,len(matriz_promedio)):\n",
    "            suma = 0\n",
    "            for j in range(1,len(matriz_promedio)):                \n",
    "                suma += matriz_promedio[i][j]\n",
    "            for j in range(1,len(matriz_promedio)):                \n",
    "                matriz_promedio[i][j] = round(float(matriz_promedio[i][j] * 100) / float(suma),2)\n",
    "        return matriz_promedio\n",
    "\n",
    "\n",
    "    def link_prediction(self,traversals,a,b,jump,metrica,filtrado):\n",
    "        # Valores para la grafica de precision en la prediccion\n",
    "        pal = pallete(\"db\")\n",
    "        X = []\n",
    "        Y = []\n",
    "        # Valores para la grafica de desviacion en la prediccion\n",
    "        Xd = []\n",
    "        Yd = []\n",
    "        i = 1\n",
    "        for i in range(a,b+1):\n",
    "            val = i * jump    \n",
    "            if self.param == \"ns\":\n",
    "                k = 3\n",
    "            if self.param == \"l\":\n",
    "                k = 3\n",
    "            if self.param == \"ndim\":\n",
    "                k = 3\n",
    "            if not (self.param == \"ns\" or self.param == \"ndim\" or self.param == \"l\"):\n",
    "                k = val\n",
    "            resultados = []   \n",
    "            if not os.path.exists(\"models/l_prediction\" + self.bd +\"ts\"+str(self.trainset_p)+self.param+str(val)+\"k\"+str(k)+\"Promedio\"+\"Metrica-\"+str(metrica)+\"Filtrado-\"+str(filtrado)+str(self.iteraciones)+\".p\") or not os.path.exists(\"models/l_prediction\" + self.bd +\"ts\"+str(self.trainset_p)+self.param+str(val)+\"k\"+str(k)+\"Resultados\"+\"Metrica-\"+str(metrica)+\"Filtrado-\"+str(filtrado)+str(self.iteraciones)+\".p\") or not os.path.exists(\"models/l_prediction\" + self.bd +\"ts\"+str(self.trainset_p)+self.param+str(val)+\"k\"+str(k)+\"MeanDev\"+\"Metrica-\"+str(metrica)+\"Filtrado-\"+str(filtrado)+str(self.iteraciones)+\".p\"):\n",
    "                final = 0\n",
    "                for it in range(self.iteraciones):\n",
    "                    if self.param == \"ns\":\n",
    "                        n2v = node2vec(self.bd,self.port,self.user,self.pss,self.label,val,200,6,self.mode,[],self.iteraciones)\n",
    "                        k = 3\n",
    "                    if self.param == \"l\":\n",
    "                        n2v = node2vec(self.bd,self.port,self.user,self.pss,self.label,400000,200,val,self.mode,[],self.iteraciones)\n",
    "                        k = 3\n",
    "                    if self.param == \"ndim\":\n",
    "                        n2v = node2vec(self.bd,self.port,self.user,self.pss,self.label,400000,val,6,self.mode,[],self.iteraciones)\n",
    "                        k = 3\n",
    "                    #si lo que vamos a estudiar no son los parametros libres de la inmersion, fijamos dichos parametros a sus valores optimos segun BD\n",
    "                    if not (self.param == \"ns\" or self.param == \"ndim\" or self.param == \"l\"):\n",
    "                        n2v = node2vec(self.bd,self.port,self.user,self.pss,self.label,optimos[self.bd][0],optimos[self.bd][1],optimos[self.bd][2],self.mode,[],self.iteraciones)\n",
    "                    n2v.learn(self.mode,self.trainset_p,True,it)\n",
    "                    total = 0\n",
    "                    parcial = 0\n",
    "                    n2v.r_analysis()\n",
    "                    if metrica == \"MRR\":\n",
    "                        if filtrado:\n",
    "                            clasificadores = {}\n",
    "                            temp_pos = {}\n",
    "                            temp_name = {}\n",
    "                            ks = {}\n",
    "                            total = 0\n",
    "                            for rt in n2v.r_deleted:\n",
    "                                temp_pos[rt] = []\n",
    "                                temp_name[rt] = []\n",
    "                                print \"Se va a comparar con: \" + str(n2v.r_deleted[rt][0][\"tipot\"])\n",
    "                                for idx,e in enumerate(n2v.nodes_type):\n",
    "                                    if e == n2v.r_deleted[rt][0][\"tipot\"]:\n",
    "                                        temp_pos[rt].append(n2v.nodes_pos[idx])\n",
    "                                        temp_name[rt].append(n2v.nodes_name[idx])\n",
    "                                if len(temp_pos[rt]) < 1000:\n",
    "                                    ks[rt] = len(temp_pos[rt])\n",
    "                                else:\n",
    "                                    ks[rt] = 1000                                \n",
    "                                clasificadores[rt] = neighbors.KNeighborsClassifier(ks[rt], \"uniform\",n_jobs=multiprocessing.cpu_count())\n",
    "                                clasificadores[rt].fit(temp_pos[rt], temp_name[rt])\n",
    "                            print \"A continuacion las aristas eliminadas\"\n",
    "                            for rt in n2v.r_deleted:\n",
    "                                targettopredict = []\n",
    "                                linkstopredictV = []\n",
    "                                for d in n2v.r_deleted[rt]:\n",
    "                                    rs = d[\"s\"]\n",
    "                                    rel = d[\"tipo\"]\n",
    "                                    tipot = d[\"tipot\"]\n",
    "                                    if rs in n2v.w2v and not '\"' in rs:\n",
    "                                        total += 1\n",
    "                                        targettopredict.append(d[\"t\"])\n",
    "                                        linkstopredictV.append(n2v.w2v[rs]+n2v.m_vectors[str(rel)])\n",
    "                                nbs = clasificadores[rt].kneighbors(linkstopredictV,ks[rt],False)\n",
    "                                for idx,e in enumerate(nbs):\n",
    "                                    nbs1 = []\n",
    "                                    for i in e:\n",
    "                                        nbs1.append(temp_name[rt][i])\n",
    "                                    if targettopredict[idx] in nbs1:\n",
    "                                        print \"ESTA EN LA LISTA DEVUELTA\"\n",
    "                                        print targettopredict[idx]\n",
    "                                        print nbs1.index(targettopredict[idx])\n",
    "                                        parcial += float(1 / float(nbs1.index(targettopredict[idx])+1 ))\n",
    "                                        print \"PUNTUACION\"\n",
    "                                        print float(1 / float(nbs1.index(targettopredict[idx])+1 ))\n",
    "                        else:\n",
    "                            clf = neighbors.KNeighborsClassifier(1000, \"uniform\",n_jobs=multiprocessing.cpu_count())\n",
    "                            clf.fit(n2v.nodes_pos, n2v.nodes_name)\n",
    "                            print \"A continuacion las aristas eliminadas\"\n",
    "                            targettopredict = []\n",
    "                            linkstopredictV = []\n",
    "                            for rt in n2v.r_deleted:\n",
    "                                for d in n2v.r_deleted[rt]:\n",
    "                                    rs = d[\"s\"]\n",
    "                                    rel = d[\"tipo\"]\n",
    "                                    tipot = d[\"tipot\"]\n",
    "                                    if rs in n2v.w2v and not '\"' in rs:\n",
    "                                        total += 1\n",
    "                                        targettopredict.append(d[\"t\"])\n",
    "                                        linkstopredictV.append(n2v.w2v[rs]+n2v.m_vectors[str(rel)])\n",
    "                            nbs = clf.kneighbors(linkstopredictV,1000,False)\n",
    "                            for idx,e in enumerate(nbs):\n",
    "                                nbs1 = []\n",
    "                                for i in e:\n",
    "                                    nbs1.append(n2v.nodes_name[i])\n",
    "                                if targettopredict[idx] in nbs1:\n",
    "                                    print \"ESTA EN LA LISTA DEVUELTA\"\n",
    "                                    print targettopredict[idx]\n",
    "                                    print nbs1.index(targettopredict[idx])\n",
    "                                    parcial += float(1 / float(nbs1.index(targettopredict[idx])+1 ))\n",
    "                                    print \"PUNTUACION\"\n",
    "                                    print float(1 / float(nbs1.index(targettopredict[idx])+1 ))\n",
    "                        if total > 0:\n",
    "                            resultIN = float(parcial)/float(total)\n",
    "                        else:\n",
    "                            resultIN = 0\n",
    "                    final += resultIN\n",
    "                    resultados.append(resultIN)\n",
    "                result = final / self.iteraciones                \n",
    "                mean_dev = 0\n",
    "                for r in resultados:\n",
    "                    mean_dev += (r - result) * (r - result)\n",
    "                mean_dev = math.sqrt(mean_dev)\n",
    "                print \"RESULTADOS DE UN PUNTO\"\n",
    "                print resultados\n",
    "                print mean_dev\n",
    "                f1 = open( \"models/l_prediction\" + self.bd +\"ts\"+str(self.trainset_p)+self.param+str(val)+\"k\"+str(k)+\"MeanDev\"+\"Metrica-\"+str(metrica)+\"Filtrado-\"+str(filtrado)+str(self.iteraciones)+\".p\", \"w\" )\n",
    "                pickle.dump(mean_dev,f1)\n",
    "                f2 = open( \"models/l_prediction\" + self.bd +\"ts\"+str(self.trainset_p)+self.param+str(val)+\"k\"+str(k)+\"Resultados\"+\"Metrica-\"+str(metrica)+\"Filtrado-\"+str(filtrado)+str(self.iteraciones)+\".p\", \"w\" )\n",
    "                pickle.dump(resultados,f2)\n",
    "                f3 = open( \"models/l_prediction\" + self.bd +\"ts\"+str(self.trainset_p)+self.param+str(val)+\"k\"+str(k)+\"Promedio\"+\"Metrica-\"+str(metrica)+\"Filtrado-\"+str(filtrado)+str(self.iteraciones)+\".p\", \"w\" )\n",
    "                pickle.dump(result,f3)\n",
    "            else:\n",
    "                f1 = open( \"models/l_prediction\" + self.bd +\"ts\"+str(self.trainset_p)+self.param+str(val)+\"k\"+str(k)+\"MeanDev\"+\"Metrica-\"+str(metrica)+\"Filtrado-\"+str(filtrado)+str(self.iteraciones)+\".p\", \"r\" )\n",
    "                mean_dev = pickle.load(f1)\n",
    "                f2 = open( \"models/l_prediction\" + self.bd +\"ts\"+str(self.trainset_p)+self.param+str(val)+\"k\"+str(k)+\"Resultados\"+\"Metrica-\"+str(metrica)+\"Filtrado-\"+str(filtrado)+str(self.iteraciones)+\".p\", \"r\" )\n",
    "                resultados = pickle.load(f2)\n",
    "                f3 = open( \"models/l_prediction\" + self.bd +\"ts\"+str(self.trainset_p)+self.param+str(val)+\"k\"+str(k)+\"Promedio\"+\"Metrica-\"+str(metrica)+\"Filtrado-\"+str(filtrado)+str(self.iteraciones)+\".p\", \"r\" )\n",
    "                result = pickle.load(f3)\n",
    "            X.append(val)\n",
    "            Y.append(result*100)\n",
    "            Xd.append(val)\n",
    "            Yd.append(mean_dev*100)\n",
    "        self.p.line(X, Y, color=pal[1],legend=\"ICH\",line_width=1.5)\n",
    "        #self.p.line(Xd, Yd, color=pal[1],legend=\"ICH\",line_width=1.5,line_dash='dotted')\n",
    "        self.p.legend.background_fill_alpha = 0.5\n",
    "        print self.bd\n",
    "        print \"max accuracy: \" + str(max(Y))\n",
    "        print \"max dev: \" + str(max(Yd))\n",
    "        return X,Y,Xd,Yd\n",
    "\n",
    "    def traversal_prediction(self,traversal,a,b,jump,metrica,filtrado):\n",
    "        # Valores para la grafica de precision en la prediccion\n",
    "        pal = pallete(\"db\")\n",
    "        X = []\n",
    "        Y = []\n",
    "        # Valores para la grafica de desviacion en la prediccion\n",
    "        Xd = []\n",
    "        Yd = []\n",
    "        i = 1\n",
    "        for i in range(a,b+1):\n",
    "            val = i * jump    \n",
    "            if self.param == \"ns\":\n",
    "                k = 3\n",
    "            if self.param == \"l\":\n",
    "                k = 3\n",
    "            if self.param == \"ndim\":\n",
    "                k = 3\n",
    "            if not (self.param == \"ns\" or self.param == \"ndim\" or self.param == \"l\"):\n",
    "                k = val\n",
    "            resultados = []   \n",
    "            if not os.path.exists(\"models/\"+traversal+\"_prediction\" + self.bd +\"ts\"+str(self.trainset_p)+self.param+str(val)+\"k\"+str(k)+\"Promedio\"+\"Metrica-\"+str(metrica)+\"Filtrado-\"+str(filtrado)+str(self.iteraciones)+\".p\") or not os.path.exists(\"models/\"+traversal+\"_prediction\" + self.bd +\"ts\"+str(self.trainset_p)+self.param+str(val)+\"k\"+str(k)+\"Resultados\"+\"Metrica-\"+str(metrica)+\"Filtrado-\"+str(filtrado)+str(self.iteraciones)+\".p\") or not os.path.exists(\"models/\"+traversal+\"_prediction\" + self.bd +\"ts\"+str(self.trainset_p)+self.param+str(val)+\"k\"+str(k)+\"MeanDev\"+\"Metrica-\"+str(metrica)+\"Filtrado-\"+str(filtrado)+str(self.iteraciones)+\".p\"):\n",
    "                final = 0\n",
    "                for it in range(self.iteraciones):\n",
    "                    if self.param == \"ns\":\n",
    "                        n2v = node2vec(self.bd,self.port,self.user,self.pss,self.label,val,200,6,self.mode,[],self.iteraciones)\n",
    "                        k = 3\n",
    "                    if self.param == \"l\":\n",
    "                        n2v = node2vec(self.bd,self.port,self.user,self.pss,self.label,400000,200,val,self.mode,[],self.iteraciones)\n",
    "                        k = 3\n",
    "                    if self.param == \"ndim\":\n",
    "                        n2v = node2vec(self.bd,self.port,self.user,self.pss,self.label,400000,val,6,self.mode,[],self.iteraciones)\n",
    "                        k = 3\n",
    "                    #si lo que vamos a estudiar no son los parametros libres de la inmersion, fijamos dichos parametros a sus valores optimos segun BD\n",
    "                    if not (self.param == \"ns\" or self.param == \"ndim\" or self.param == \"l\"):\n",
    "                        n2v = node2vec(self.bd,self.port,self.user,self.pss,self.label,optimos[self.bd][0],optimos[self.bd][1],optimos[self.bd][2],self.mode,[],self.iteraciones)\n",
    "                    n2v.learn(self.mode,0,False,it)\n",
    "                    parcial = 0\n",
    "                    n2v.r_analysis()\n",
    "                    #Obtenemos el vector medio del traversal solicitado. \n",
    "                    v_traversal = n2v.get_vtraversal(traversal)\n",
    "                    #Obtenemos una serie de traversals por los que vamos a preguntar. Es una lista que contiene diccionarios con: nodo origen (s), nodo destino (t) y tipo del nodo destino (tipot).\n",
    "                    traversals = n2v.get_traversals(traversal,self.trainset_p)\n",
    "                    if metrica == \"MRR\":\n",
    "                        if filtrado:\n",
    "                            temp_pos = []\n",
    "                            temp_name = []\n",
    "                            print \"Se va a comparar con: \" + str(traversals[0][\"tipot\"])\n",
    "                            for idx,e in enumerate(n2v.nodes_type):\n",
    "                                if e == traversals[0][\"tipot\"]:\n",
    "                                    temp_pos.append(n2v.nodes_pos[idx])\n",
    "                                    temp_name.append(n2v.nodes_name[idx])\n",
    "                            if len(temp_pos) < 1000:\n",
    "                                ks = len(temp_pos)\n",
    "                            else:\n",
    "                                ks = 1000                                \n",
    "                            clasificador = neighbors.KNeighborsClassifier(ks, \"uniform\",n_jobs=multiprocessing.cpu_count())\n",
    "                            clasificador.fit(temp_pos, temp_name)\n",
    "                            print \"A continuacion la verificacion de traversals\"\n",
    "                            targettopredict = []\n",
    "                            linkstopredictV = []\n",
    "                            for t in traversals:\n",
    "                                rs = t[\"s\"]\n",
    "                                tipot = t[\"tipot\"]\n",
    "                                if rs in n2v.w2v and not '\"' in rs:\n",
    "                                    targettopredict.append(t[\"t\"])\n",
    "                                    linkstopredictV.append(n2v.w2v[rs]+v_traversal)\n",
    "                            print \"Tamanio del conjunto de entrenamiento\"\n",
    "                            print len(linkstopredictV)\n",
    "                            total = len(linkstopredictV)\n",
    "                            nbs = clasificador.kneighbors(linkstopredictV,ks,False)\n",
    "                            for idx,e in enumerate(nbs):\n",
    "                                nbs1 = []\n",
    "                                for i in e:\n",
    "                                    nbs1.append(temp_name[i])\n",
    "                                if targettopredict[idx] in nbs1:\n",
    "                                    print \"ESTA EN LA LISTA DEVUELTA\"\n",
    "                                    print targettopredict[idx]\n",
    "                                    print nbs1.index(targettopredict[idx])\n",
    "                                    parcial += float(1 / float(nbs1.index(targettopredict[idx])+1 ))\n",
    "                                    print \"PUNTUACION\"\n",
    "                                    print float(1 / float(nbs1.index(targettopredict[idx])+1 ))\n",
    "                        else:\n",
    "                            clf = neighbors.KNeighborsClassifier(1000, \"uniform\",n_jobs=multiprocessing.cpu_count())\n",
    "                            clf.fit(n2v.nodes_pos, n2v.nodes_name)\n",
    "                            print \"A continuacion la verificacion de traversals\"\n",
    "                            targettopredict = []\n",
    "                            linkstopredictV = []\n",
    "                            for t in traversals:\n",
    "                                rs = t[\"s\"]\n",
    "                                tipot = t[\"tipot\"]\n",
    "                                if rs in n2v.w2v and not '\"' in rs:\n",
    "                                    targettopredict.append(t[\"t\"])\n",
    "                                    linkstopredictV.append(n2v.w2v[rs]+v_traversal)\n",
    "                            total = len(linkstopredictV)\n",
    "                            nbs = clf.kneighbors(linkstopredictV,1000,False)\n",
    "                            for idx,e in enumerate(nbs):\n",
    "                                nbs1 = []\n",
    "                                for i in e:\n",
    "                                    nbs1.append(n2v.nodes_name[i])\n",
    "                                if targettopredict[idx] in nbs1:\n",
    "                                    print \"ESTA EN LA LISTA DEVUELTA\"\n",
    "                                    print targettopredict[idx]\n",
    "                                    print nbs1.index(targettopredict[idx])\n",
    "                                    parcial += float(1 / float(nbs1.index(targettopredict[idx])+1 ))\n",
    "                                    print \"PUNTUACION\"\n",
    "                                    print float(1 / float(nbs1.index(targettopredict[idx])+1 ))\n",
    "                        if total > 0:\n",
    "                            print parcial\n",
    "                            print total\n",
    "                            resultIN = float(parcial)/float(total)\n",
    "                            print resultIN\n",
    "                        else:\n",
    "                            resultIN = 0\n",
    "                    final += resultIN\n",
    "                    resultados.append(resultIN)\n",
    "                result = final / self.iteraciones                \n",
    "                mean_dev = 0\n",
    "                for r in resultados:\n",
    "                    mean_dev += (r - result) * (r - result)\n",
    "                mean_dev = math.sqrt(mean_dev)\n",
    "                print \"RESULTADOS DE UN PUNTO\"\n",
    "                print resultados\n",
    "                print mean_dev\n",
    "                f1 = open( \"models/\"+traversal+\"_prediction\" + self.bd +\"ts\"+str(self.trainset_p)+self.param+str(val)+\"k\"+str(k)+\"MeanDev\"+\"Metrica-\"+str(metrica)+\"Filtrado-\"+str(filtrado)+str(self.iteraciones)+\".p\", \"w\" )\n",
    "                pickle.dump(mean_dev,f1)\n",
    "                f2 = open( \"models/\"+traversal+\"_prediction\" + self.bd +\"ts\"+str(self.trainset_p)+self.param+str(val)+\"k\"+str(k)+\"Resultados\"+\"Metrica-\"+str(metrica)+\"Filtrado-\"+str(filtrado)+str(self.iteraciones)+\".p\", \"w\" )\n",
    "                pickle.dump(resultados,f2)\n",
    "                f3 = open( \"models/\"+traversal+\"_prediction\" + self.bd +\"ts\"+str(self.trainset_p)+self.param+str(val)+\"k\"+str(k)+\"Promedio\"+\"Metrica-\"+str(metrica)+\"Filtrado-\"+str(filtrado)+str(self.iteraciones)+\".p\", \"w\" )\n",
    "                pickle.dump(result,f3)\n",
    "            else:\n",
    "                f1 = open( \"models/\"+traversal+\"_prediction\" + self.bd +\"ts\"+str(self.trainset_p)+self.param+str(val)+\"k\"+str(k)+\"MeanDev\"+\"Metrica-\"+str(metrica)+\"Filtrado-\"+str(filtrado)+str(self.iteraciones)+\".p\", \"r\" )\n",
    "                mean_dev = pickle.load(f1)\n",
    "                f2 = open( \"models/\"+traversal+\"_prediction\" + self.bd +\"ts\"+str(self.trainset_p)+self.param+str(val)+\"k\"+str(k)+\"Resultados\"+\"Metrica-\"+str(metrica)+\"Filtrado-\"+str(filtrado)+str(self.iteraciones)+\".p\", \"r\" )\n",
    "                resultados = pickle.load(f2)\n",
    "                f3 = open( \"models/\"+traversal+\"_prediction\" + self.bd +\"ts\"+str(self.trainset_p)+self.param+str(val)+\"k\"+str(k)+\"Promedio\"+\"Metrica-\"+str(metrica)+\"Filtrado-\"+str(filtrado)+str(self.iteraciones)+\".p\", \"r\" )\n",
    "                result = pickle.load(f3)\n",
    "            X.append(val)\n",
    "            Y.append(result*100)\n",
    "            Xd.append(val)\n",
    "            Yd.append(mean_dev*100)\n",
    "        self.p.line(X, Y, color=pal[1],legend=\"ICH\",line_width=1.5)\n",
    "        #self.p.line(Xd, Yd, color=pal[1],legend=\"ICH\",line_width=1.5,line_dash='dotted')\n",
    "        self.p.legend.background_fill_alpha = 0.5\n",
    "        print self.bd\n",
    "        print \"YOO\"\n",
    "        print \"max accuracy: \" + str(max(Y))\n",
    "        print \"max dev: \" + str(max(Yd))\n",
    "        return X,Y,Xd,Yd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch_sparse'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-da8730dfc393>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmanifold\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mTSNE\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtorch_geometric\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdatasets\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mPlanetoid\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtorch_geometric\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mNode2Vec\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch_geometric\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mdebug\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mis_debug_enabled\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdebug\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mset_debug\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtorch_geometric\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtorch_geometric\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtorch_geometric\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch_geometric\\data\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mData\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mtemporal\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mTemporalData\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mbatch\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mBatch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDataset\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0min_memory_dataset\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mInMemoryDataset\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch_geometric\\data\\data.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtorch_geometric\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtorch_sparse\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcoalesce\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSparseTensor\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m from torch_geometric.utils import (contains_isolated_nodes,\n\u001b[0;32m     10\u001b[0m                                    contains_self_loops, is_undirected)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torch_sparse'"
     ]
    }
   ],
   "source": [
    "import os.path as osp\n",
    "\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "from torch_geometric.datasets import Planetoid\n",
    "from torch_geometric.nn import Node2Vec\n",
    "\n",
    "\n",
    "def main():\n",
    "    dataset = 'Cora'\n",
    "    path = osp.join(osp.dirname(osp.realpath(__file__)), '..', 'data', dataset)\n",
    "    dataset = Planetoid(path, dataset)\n",
    "    data = dataset[0]\n",
    "\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    model = Node2Vec(data.edge_index, embedding_dim=128, walk_length=20,\n",
    "                     context_size=10, walks_per_node=10,\n",
    "                     num_negative_samples=1, p=1, q=1, sparse=True).to(device)\n",
    "\n",
    "    loader = model.loader(batch_size=128, shuffle=True, num_workers=4)\n",
    "    optimizer = torch.optim.SparseAdam(list(model.parameters()), lr=0.01)\n",
    "\n",
    "    def train():\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for pos_rw, neg_rw in loader:\n",
    "            optimizer.zero_grad()\n",
    "            loss = model.loss(pos_rw.to(device), neg_rw.to(device))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        return total_loss / len(loader)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def test():\n",
    "        model.eval()\n",
    "        z = model()\n",
    "        acc = model.test(z[data.train_mask], data.y[data.train_mask],\n",
    "                         z[data.test_mask], data.y[data.test_mask],\n",
    "                         max_iter=150)\n",
    "        return acc\n",
    "\n",
    "    for epoch in range(1, 101):\n",
    "        loss = train()\n",
    "        acc = test()\n",
    "        print(f'Epoch: {epoch:02d}, Loss: {loss:.4f}, Acc: {acc:.4f}')\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def plot_points(colors):\n",
    "        model.eval()\n",
    "        z = model(torch.arange(data.num_nodes, device=device))\n",
    "        z = TSNE(n_components=2).fit_transform(z.cpu().numpy())\n",
    "        y = data.y.cpu().numpy()\n",
    "\n",
    "        plt.figure(figsize=(8, 8))\n",
    "        for i in range(dataset.num_classes):\n",
    "            plt.scatter(z[y == i, 0], z[y == i, 1], s=20, color=colors[i])\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "\n",
    "    colors = [\n",
    "        '#ffc0cb', '#bada55', '#008080', '#420420', '#7fe5f0', '#065535',\n",
    "        '#ffd700'\n",
    "    ]\n",
    "    plot_points(colors)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[WinError 127] Не найдена указанная процедура",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-7708a88d2121>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtorch_sparse\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSparseTensor\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear_model\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch_sparse\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[1;34m'_saint'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'_sample'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'_relabel'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m ]:\n\u001b[1;32m---> 14\u001b[1;33m     torch.ops.load_library(importlib.machinery.PathFinder().find_spec(\n\u001b[0m\u001b[0;32m     15\u001b[0m         f'{library}_{suffix}', [osp.dirname(__file__)]).origin)\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\_ops.py\u001b[0m in \u001b[0;36mload_library\u001b[1;34m(self, path)\u001b[0m\n\u001b[0;32m    102\u001b[0m             \u001b[1;31m# static (global) initialization code in order to register custom\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    103\u001b[0m             \u001b[1;31m# operators with the JIT.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 104\u001b[1;33m             \u001b[0mctypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCDLL\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    105\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloaded_libraries\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    106\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\ctypes\\__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, name, mode, handle, use_errno, use_last_error, winmode)\u001b[0m\n\u001b[0;32m    379\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    380\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 381\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_dlopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    382\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    383\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mOSError\u001b[0m: [WinError 127] Не найдена указанная процедура"
     ]
    }
   ],
   "source": [
    "from torch.nn import Embedding\n",
    "import tensorflow as tf\n",
    "from torch.utils.data import DataLoader\n",
    "from torch_sparse import SparseTensor\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from torch_geometric.utils.num_nodes import maybe_num_nodes\n",
    "\n",
    "try:\n",
    "    import torch_cluster  # noqa\n",
    "    random_walk = torch.ops.torch_cluster.random_walk\n",
    "except ImportError:\n",
    "    random_walk = None\n",
    "\n",
    "EPS = 1e-15\n",
    "\n",
    "\n",
    "class Node2Vec(torch.nn.Module):\n",
    "    r\"\"\"The Node2Vec model from the\n",
    "    `\"node2vec: Scalable Feature Learning for Networks\"\n",
    "    <https://arxiv.org/abs/1607.00653>`_ paper where random walks of\n",
    "    length :obj:`walk_length` are sampled in a given graph, and node embeddings\n",
    "    are learned via negative sampling optimization.\n",
    "\n",
    "    .. note::\n",
    "\n",
    "        For an example of using Node2Vec, see `examples/node2vec.py\n",
    "        <https://github.com/rusty1s/pytorch_geometric/blob/master/examples/\n",
    "        node2vec.py>`_.\n",
    "\n",
    "    Args:\n",
    "        edge_index (LongTensor): The edge indices.\n",
    "        embedding_dim (int): The size of each embedding vector.\n",
    "        walk_length (int): The walk length.\n",
    "        context_size (int): The actual context size which is considered for\n",
    "            positive samples. This parameter increases the effective sampling\n",
    "            rate by reusing samples across different source nodes.\n",
    "        walks_per_node (int, optional): The number of walks to sample for each\n",
    "            node. (default: :obj:`1`)\n",
    "        p (float, optional): Likelihood of immediately revisiting a node in the\n",
    "            walk. (default: :obj:`1`)\n",
    "        q (float, optional): Control parameter to interpolate between\n",
    "            breadth-first strategy and depth-first strategy (default: :obj:`1`)\n",
    "        num_negative_samples (int, optional): The number of negative samples to\n",
    "            use for each positive sample. (default: :obj:`1`)\n",
    "        num_nodes (int, optional): The number of nodes. (default: :obj:`None`)\n",
    "        sparse (bool, optional): If set to :obj:`True`, gradients w.r.t. to the\n",
    "            weight matrix will be sparse. (default: :obj:`False`)\n",
    "    \"\"\"\n",
    "    def __init__(self, edge_index, embedding_dim, walk_length, context_size,\n",
    "                 walks_per_node=1, p=1, q=1, num_negative_samples=1,\n",
    "                 num_nodes=None, sparse=False):\n",
    "        super(Node2Vec, self).__init__()\n",
    "\n",
    "        if random_walk is None:\n",
    "            raise ImportError('`Node2Vec` requires `torch-cluster`.')\n",
    "\n",
    "        N = maybe_num_nodes(edge_index, num_nodes)\n",
    "        row, col = edge_index\n",
    "        self.adj = SparseTensor(row=row, col=col, sparse_sizes=(N, N))\n",
    "        self.adj = self.adj.to('cpu')\n",
    "\n",
    "        assert walk_length >= context_size\n",
    "\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.walk_length = walk_length - 1\n",
    "        self.context_size = context_size\n",
    "        self.walks_per_node = walks_per_node\n",
    "        self.p = p\n",
    "        self.q = q\n",
    "        self.num_negative_samples = num_negative_samples\n",
    "\n",
    "        self.embedding = Embedding(N, embedding_dim, sparse=sparse)\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.embedding.reset_parameters()\n",
    "\n",
    "\n",
    "    def forward(self, batch=None):\n",
    "        \"\"\"Returns the embeddings for the nodes in :obj:`batch`.\"\"\"\n",
    "        emb = self.embedding.weight\n",
    "        return emb if batch is None else emb[batch]\n",
    "\n",
    "\n",
    "    def loader(self, **kwargs):\n",
    "        return DataLoader(range(self.adj.sparse_size(0)),\n",
    "                          collate_fn=self.sample, **kwargs)\n",
    "\n",
    "\n",
    "    def pos_sample(self, batch):\n",
    "        batch = batch.repeat(self.walks_per_node)\n",
    "        rowptr, col, _ = self.adj.csr()\n",
    "        rw = random_walk(rowptr, col, batch, self.walk_length, self.p, self.q)\n",
    "        if not isinstance(rw, torch.Tensor):\n",
    "            rw = rw[0]\n",
    "\n",
    "        walks = []\n",
    "        num_walks_per_rw = 1 + self.walk_length + 1 - self.context_size\n",
    "        for j in range(num_walks_per_rw):\n",
    "            walks.append(rw[:, j:j + self.context_size])\n",
    "        return torch.cat(walks, dim=0)\n",
    "\n",
    "\n",
    "    def neg_sample(self, batch):\n",
    "        batch = batch.repeat(self.walks_per_node * self.num_negative_samples)\n",
    "\n",
    "        rw = torch.randint(self.adj.sparse_size(0),\n",
    "                           (batch.size(0), self.walk_length))\n",
    "        rw = torch.cat([batch.view(-1, 1), rw], dim=-1)\n",
    "\n",
    "        walks = []\n",
    "        num_walks_per_rw = 1 + self.walk_length + 1 - self.context_size\n",
    "        for j in range(num_walks_per_rw):\n",
    "            walks.append(rw[:, j:j + self.context_size])\n",
    "        return torch.cat(walks, dim=0)\n",
    "\n",
    "\n",
    "    def sample(self, batch):\n",
    "        if not isinstance(batch, torch.Tensor):\n",
    "            batch = torch.tensor(batch)\n",
    "        return self.pos_sample(batch), self.neg_sample(batch)\n",
    "\n",
    "\n",
    "    def loss(self, pos_rw, neg_rw):\n",
    "        r\"\"\"Computes the loss given positive and negative random walks.\"\"\"\n",
    "\n",
    "        # Positive loss.\n",
    "        start, rest = pos_rw[:, 0], pos_rw[:, 1:].contiguous()\n",
    "\n",
    "        h_start = self.embedding(start).view(pos_rw.size(0), 1,\n",
    "                                             self.embedding_dim)\n",
    "        h_rest = self.embedding(rest.view(-1)).view(pos_rw.size(0), -1,\n",
    "                                                    self.embedding_dim)\n",
    "\n",
    "        out = (h_start * h_rest).sum(dim=-1).view(-1)\n",
    "        pos_loss = -torch.log(torch.sigmoid(out) + EPS).mean()\n",
    "\n",
    "        # Negative loss.\n",
    "        start, rest = neg_rw[:, 0], neg_rw[:, 1:].contiguous()\n",
    "\n",
    "        h_start = self.embedding(start).view(neg_rw.size(0), 1,\n",
    "                                             self.embedding_dim)\n",
    "        h_rest = self.embedding(rest.view(-1)).view(neg_rw.size(0), -1,\n",
    "                                                    self.embedding_dim)\n",
    "\n",
    "        out = (h_start * h_rest).sum(dim=-1).view(-1)\n",
    "        neg_loss = -torch.log(1 - torch.sigmoid(out) + EPS).mean()\n",
    "\n",
    "        return pos_loss + neg_loss\n",
    "\n",
    "\n",
    "    def test(self, train_z, train_y, test_z, test_y, solver='lbfgs',\n",
    "             multi_class='auto', *args, **kwargs):\n",
    "        r\"\"\"Evaluates latent space quality via a logistic regression downstream\n",
    "        task.\"\"\"\n",
    "        clf = LogisticRegression(solver=solver, multi_class=multi_class, *args,\n",
    "                                 **kwargs).fit(train_z.detach().cpu().numpy(),\n",
    "                                               train_y.detach().cpu().numpy())\n",
    "        return clf.score(test_z.detach().cpu().numpy(),\n",
    "                         test_y.detach().cpu().numpy())\n",
    "\n",
    "\n",
    "    def __repr__(self):\n",
    "        return '{}({}, {})'.format(self.__class__.__name__,\n",
    "                                   self.embedding.weight.size(0),\n",
    "                                   self.embedding.weight.size(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[WinError 127] Не найдена указанная процедура",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-1a2118ad176e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtorch_sparse\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSparseTensor\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch_sparse\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[1;34m'_saint'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'_sample'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'_relabel'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m ]:\n\u001b[1;32m---> 14\u001b[1;33m     torch.ops.load_library(importlib.machinery.PathFinder().find_spec(\n\u001b[0m\u001b[0;32m     15\u001b[0m         f'{library}_{suffix}', [osp.dirname(__file__)]).origin)\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\_ops.py\u001b[0m in \u001b[0;36mload_library\u001b[1;34m(self, path)\u001b[0m\n\u001b[0;32m    102\u001b[0m             \u001b[1;31m# static (global) initialization code in order to register custom\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    103\u001b[0m             \u001b[1;31m# operators with the JIT.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 104\u001b[1;33m             \u001b[0mctypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCDLL\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    105\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloaded_libraries\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    106\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\ctypes\\__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, name, mode, handle, use_errno, use_last_error, winmode)\u001b[0m\n\u001b[0;32m    379\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    380\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 381\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_dlopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    382\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    383\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mOSError\u001b[0m: [WinError 127] Не найдена указанная процедура"
     ]
    }
   ],
   "source": [
    "from torch_sparse import SparseTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[WinError 127] Не найдена указанная процедура",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-da8730dfc393>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmanifold\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mTSNE\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtorch_geometric\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdatasets\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mPlanetoid\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtorch_geometric\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mNode2Vec\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch_geometric\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mdebug\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mis_debug_enabled\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdebug\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mset_debug\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtorch_geometric\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtorch_geometric\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtorch_geometric\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch_geometric\\data\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mData\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mtemporal\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mTemporalData\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mbatch\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mBatch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDataset\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0min_memory_dataset\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mInMemoryDataset\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch_geometric\\data\\data.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtorch_geometric\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtorch_sparse\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcoalesce\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSparseTensor\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m from torch_geometric.utils import (contains_isolated_nodes,\n\u001b[0;32m     10\u001b[0m                                    contains_self_loops, is_undirected)\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch_sparse\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[1;34m'_saint'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'_sample'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'_relabel'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m ]:\n\u001b[1;32m---> 14\u001b[1;33m     torch.ops.load_library(importlib.machinery.PathFinder().find_spec(\n\u001b[0m\u001b[0;32m     15\u001b[0m         f'{library}_{suffix}', [osp.dirname(__file__)]).origin)\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\_ops.py\u001b[0m in \u001b[0;36mload_library\u001b[1;34m(self, path)\u001b[0m\n\u001b[0;32m    102\u001b[0m             \u001b[1;31m# static (global) initialization code in order to register custom\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    103\u001b[0m             \u001b[1;31m# operators with the JIT.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 104\u001b[1;33m             \u001b[0mctypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCDLL\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    105\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloaded_libraries\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    106\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\ctypes\\__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, name, mode, handle, use_errno, use_last_error, winmode)\u001b[0m\n\u001b[0;32m    379\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    380\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 381\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_dlopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    382\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    383\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mOSError\u001b[0m: [WinError 127] Не найдена указанная процедура"
     ]
    }
   ],
   "source": [
    "import os.path as osp\n",
    "\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "from torch_geometric.datasets import Planetoid\n",
    "from torch_geometric.nn import Node2Vec\n",
    "\n",
    "\n",
    "def main():\n",
    "    dataset = 'Cora'\n",
    "    path = osp.join(osp.dirname(osp.realpath(__file__)), '..', 'data', dataset)\n",
    "    dataset = Planetoid(path, dataset)\n",
    "    data = dataset[0]\n",
    "\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    model = Node2Vec(data.edge_index, embedding_dim=128, walk_length=20,\n",
    "                     context_size=10, walks_per_node=10,\n",
    "                     num_negative_samples=1, p=1, q=1, sparse=True).to(device)\n",
    "\n",
    "    loader = model.loader(batch_size=128, shuffle=True, num_workers=4)\n",
    "    optimizer = torch.optim.SparseAdam(list(model.parameters()), lr=0.01)\n",
    "\n",
    "    def train():\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for pos_rw, neg_rw in loader:\n",
    "            optimizer.zero_grad()\n",
    "            loss = model.loss(pos_rw.to(device), neg_rw.to(device))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        return total_loss / len(loader)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def test():\n",
    "        model.eval()\n",
    "        z = model()\n",
    "        acc = model.test(z[data.train_mask], data.y[data.train_mask],\n",
    "                         z[data.test_mask], data.y[data.test_mask],\n",
    "                         max_iter=150)\n",
    "        return acc\n",
    "\n",
    "    for epoch in range(1, 101):\n",
    "        loss = train()\n",
    "        acc = test()\n",
    "        print(f'Epoch: {epoch:02d}, Loss: {loss:.4f}, Acc: {acc:.4f}')\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def plot_points(colors):\n",
    "        model.eval()\n",
    "        z = model(torch.arange(data.num_nodes, device=device))\n",
    "        z = TSNE(n_components=2).fit_transform(z.cpu().numpy())\n",
    "        y = data.y.cpu().numpy()\n",
    "\n",
    "        plt.figure(figsize=(8, 8))\n",
    "        for i in range(dataset.num_classes):\n",
    "            plt.scatter(z[y == i, 0], z[y == i, 1], s=20, color=colors[i])\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "\n",
    "    colors = [\n",
    "        '#ffc0cb', '#bada55', '#008080', '#420420', '#7fe5f0', '#065535',\n",
    "        '#ffd700'\n",
    "    ]\n",
    "    plot_points(colors)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install -U matplotlib\n",
    "import matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
